{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__          import division\n",
    "from scipy.stats         import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import array\n",
    "import math\n",
    "import struct\n",
    "import random\n",
    "\n",
    "TRAIN_IMAGES  = \"C:\\\\Users\\\\oop\\\\Desktop\\\\Winter 2016\\\\train-images.idx3-ubyte\"\n",
    "TRAIN_LABELS  = \"C:\\\\Users\\\\oop\\\\Desktop\\\\Winter 2016\\\\train-labels.idx1-ubyte\"\n",
    "TEST_IMAGES = \"C:\\\\Users\\\\oop\\\\Desktop\\\\Winter 2016\\\\t10k-images.idx3-ubyte\"\n",
    "TEST_LABELS  = \"C:\\\\Users\\\\oop\\\\Desktop\\\\Winter 2016\\\\t10k-labels.idx1-ubyte\"\n",
    "\n",
    "def read_mnist(images_file, labels_file): \n",
    "    f1 = open(labels_file, 'rb')\n",
    "    magic_number, size = struct.unpack(\">II\", f1.read(8))\n",
    "    labels = array.array(\"b\", f1.read())\n",
    "    f1.close()\n",
    "    \n",
    "    f2 = open(images_file, 'rb')\n",
    "    magic_number, size, rows, cols = struct.unpack(\">IIII\", f2.read(16))\n",
    "    raw_images = array.array(\"B\", f2.read())\n",
    "    f2.close()\n",
    "\n",
    "    N = len(labels)\n",
    "    images = np.zeros((N, rows*cols), dtype=np.uint8)\n",
    "    for i in range(N):\n",
    "        images[i] = np.array(raw_images[ i*rows*cols : (i+1)*rows*cols ])\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def sigmoid(x):\n",
    "    if type(x) is np.ndarray or type(x) is list:\n",
    "        return np.array([sigmoid(ele) for ele in x])\n",
    "    else:\n",
    "        return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivate(x):\n",
    "    if type(x) is np.ndarray or type(x) is list:\n",
    "        return np.array([sigmoid_derivate(ele) for ele in x])\n",
    "    else:\n",
    "        a = sigmoid(x)\n",
    "        return a * (1-a)\n",
    "\n",
    "def tanhx_activation_fn(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanhx_activation_fn_derivative(x):\n",
    "    return 1 - (np.tanh(x) ** 2)\n",
    "\n",
    "def max_activation_fn(x):\n",
    "    if type(x) is np.ndarray or type(x) is list:\n",
    "        return np.array([max(0, ele) for ele in x])\n",
    "    else:\n",
    "        return max(0, x)\n",
    "\n",
    "def max_activation_fn_derivative(x):\n",
    "    if type(x) is np.ndarray or type(x) is list:\n",
    "        return np.array([1 if ele > 0 else 0 for ele in x])\n",
    "    else:\n",
    "        return 1 if x > 0 else 0\n",
    "    \n",
    "class MultiLayerNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, inputs, outputs, learning_rate, layers, \n",
    "                 activation_fn, activation_derivative_fn, \n",
    "                 validation_size, *args, **kwargs):\n",
    "        train_data_size = len(inputs) - validation_size\n",
    "        self.inputs = inputs[:train_data_size]\n",
    "        self.outputs = outputs[:train_data_size]\n",
    "        self.cross_validation_inputs = inputs[train_data_size:]\n",
    "        self.cross_validation_outputs = outputs[train_data_size:]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = layers\n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_derivative_fn = activation_derivative_fn\n",
    "    \n",
    "    def get_zero_weights(self):\n",
    "        weights = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            weights.append(np.zeros((self.layers[i]+1, \n",
    "                                     self.layers[i+1])))\n",
    "        return weights\n",
    "    \n",
    "    def get_random_weights(self):\n",
    "        weights = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            weights.append(0.1 * np.random.random((self.layers[i]+1, \n",
    "                                             self.layers[i+1])))\n",
    "        return weights\n",
    "    \n",
    "    def get_gradients(self, X, Y, weights):\n",
    "        # Forward propogation.\n",
    "        a,z = self.forward_prop(X, Y, weights)\n",
    "            \n",
    "        # Backward error propogation.\n",
    "        deltas = []\n",
    "        deltas.append((Y - a[-1]))\n",
    "        for l in reversed(range(1, len(self.layers)-1)):\n",
    "            g = np.apply_along_axis(self.activation_derivative_fn, axis=1, arr=z[l])\n",
    "            delta = np.matrix(np.array(np.matmul(deltas[-1], weights[l].transpose()))*np.array(g))\n",
    "            deltas.append(np.delete(delta, 0, axis=1)) # Note that we remove deltas calculated for bias node.\n",
    "        deltas.reverse()\n",
    "            \n",
    "        gradients = []\n",
    "        for i in range(len(weights)):\n",
    "            gradients.append(np.matmul(a[i].transpose(), deltas[i]))\n",
    "        return gradients\n",
    "            \n",
    "    def train(self, \n",
    "              weights=None, \n",
    "              max_iterations=10000, \n",
    "              momentum=0.95, \n",
    "              lambda_val=0.001,\n",
    "              batch_learning=False,\n",
    "              mini_batch_learning=True,\n",
    "              online_learning=False):\n",
    "        \"\"\"\n",
    "        Trains the data using multilayered.\n",
    "        \"\"\"\n",
    "        weights     = weights or self.get_random_weights()\n",
    "        prev_weights = weights\n",
    "        train_error = []\n",
    "        cross_validation_error = []\n",
    "        test_error  = []\n",
    "        min_cross_validation_error = float(\"inf\")\n",
    "        optimum_weights = None\n",
    "        count = 0\n",
    "        k = 0\n",
    "        for itr in range(0, max_iterations+1):\n",
    "            if itr != 0:\n",
    "                X = self.inputs\n",
    "                Y = self.inputs\n",
    "                if online_learning:\n",
    "                    i = random.randint(0,len(self.inputs)-1)\n",
    "                    X = self.inputs[i]\n",
    "                    Y = self.outputs[i]\n",
    "                    \n",
    "                if mini_batch_learning:\n",
    "                    # Mini Batch learning.\n",
    "                    batch = list(set([random.randint(0,len(self.inputs)-1) \n",
    "                                      for i in range(500)]))\n",
    "                    X = []\n",
    "                    Y = []\n",
    "                    for i in batch:\n",
    "                        X.append(self.inputs[i])\n",
    "                        Y.append(self.outputs[i])\n",
    "                    X = np.matrix(np.array(X))\n",
    "                    Y = np.matrix(np.array(Y))\n",
    "                \n",
    "                # Use gradient descent algorithm to update\n",
    "                # accordingly due to error derivatives.\n",
    "                gradients = self.get_gradients(X, Y, weights)\n",
    "                for i in range(len(weights)):\n",
    "                    gradienti = gradients[i] + lambda_val * weights[i]\n",
    "                    new_weights     = (weights[i] + (self.learning_rate * gradienti)\n",
    "                                       + (momentum * (weights[i] - prev_weights[i])))\n",
    "                    prev_weights[i] = weights[i]\n",
    "                    weights[i]      = new_weights\n",
    "            \n",
    "        \n",
    "            error = self.test(self.inputs, \n",
    "                              self.outputs,\n",
    "                              weights)\n",
    "            \n",
    "            train_error.append(error)\n",
    "\n",
    "            error = self.test(self.cross_validation_inputs, \n",
    "                              self.cross_validation_outputs,\n",
    "                              weights)\n",
    "            if error < min_cross_validation_error:\n",
    "                min_cross_validation_error = error\n",
    "                count = 0\n",
    "                optimum_weights = weights\n",
    "            else:\n",
    "                count += 1\n",
    "            cross_validation_error.append(error)\n",
    "\n",
    "            error = self.test(X_test, \n",
    "                              Y_test,\n",
    "                              weights)\n",
    "            test_error.append(error)\n",
    "            \n",
    "            print train_error[-1], cross_validation_error[-1], test_error[-1]\n",
    "\n",
    "            # Validation set error is increasing.\n",
    "            if count > 10:\n",
    "                break\n",
    "                \n",
    "        self.weights     = optimum_weights\n",
    "        self.train_error = train_error\n",
    "        self.test_error  = test_error\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def test_gradient(self):\n",
    "        weights = self.get_random_weights()\n",
    "        gradients = self.get_gradients(self.inputs,\n",
    "                                       self.outputs, \n",
    "                                       weights)\n",
    "        \n",
    "        epsilon = 10**-5\n",
    "        approximate_gradients = self.get_random_weights()\n",
    "        for l in range(len(self.layers)-1):\n",
    "            for i in range(len(weights[l])):\n",
    "                for j in range(len(weights[l][i])):\n",
    "                    w = weights[l][i][j]\n",
    "                    weights[l][i][j] = w + epsilon\n",
    "                    val1 = self.cross_entropy(weights)\n",
    "                    weights[l][i][j] = w - epsilon\n",
    "                    val2 = self.cross_entropy(weights)\n",
    "                    weights[l][i][j] = w\n",
    "                    approximate_gradients[l][i][j] = (val1 - val2) / (2*epsilon)\n",
    "        \n",
    "        # Find the min,max and avg difference \n",
    "        # between actual and approximate gradient\n",
    "        no_of_units = 0\n",
    "        diff_sum = 0\n",
    "        max_diff = float(\"-inf\")\n",
    "        min_diff = float(\"inf\")\n",
    "        for l in range(len(gradients)):\n",
    "            g = np.abs(gradients[l].flatten())\n",
    "            a = np.abs(approximate_gradients[l].flatten())\n",
    "            diff = np.abs(g-a)\n",
    "            max_diff = max(max_diff, diff.max())\n",
    "            min_diff = min(min_diff, diff.min())\n",
    "            diff_sum += diff.sum()\n",
    "            no_of_units += g.shape[0] * g.shape[1]\n",
    "        avg_diff = diff_sum / no_of_units\n",
    "        return max_diff, min_diff, avg_diff\n",
    "            \n",
    "    def cross_entropy(self, weights):\n",
    "        a,z = self.forward_prop(self.inputs, self.outputs, weights)\n",
    "        return -np.multiply(np.log(a[-1]), self.outputs).sum()\n",
    "\n",
    "    def forward_prop(self, inputs, outputs, weights):\n",
    "        \"\"\"\n",
    "        Calculates the output at each layer of the network.\n",
    "        \"\"\"\n",
    "        a = [np.insert(inputs, 0, 1, axis=1)]\n",
    "        z = [np.insert(inputs, 0, 1, axis=1)]\n",
    "        for l in range(len(self.layers)-1):\n",
    "            zl = np.matmul(z[l], weights[l])\n",
    "            if l == (len(self.layers)-2):\n",
    "                output = np.exp(zl)\n",
    "                output = output / output.sum(axis=1)\n",
    "                a.append(output)\n",
    "                z.append(zl)\n",
    "            else:\n",
    "                z.append(np.insert(zl, 0, 1, axis=1))\n",
    "                al = np.apply_along_axis(self.activation_fn, axis=1, arr=zl)\n",
    "                a.append(np.insert(al, 0, 1, axis=1))\n",
    "        return a,z\n",
    "    \n",
    "    def test(self, test_inputs, test_outputs, weights):\n",
    "        a,z = self.forward_prop(test_inputs, test_outputs, weights)\n",
    "        predicted_digits = a[-1].argmax(axis=1)\n",
    "        actual_digits = test_outputs.argmax(axis=1)\n",
    "        error = (predicted_digits != actual_digits).sum()\n",
    "        return error * 100 / len(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read training data.\n",
    "images_train, labels_train = read_mnist(TRAIN_IMAGES, TRAIN_LABELS)\n",
    "\n",
    "# Read Test data.\n",
    "images_test, labels_test = read_mnist(TEST_IMAGES, TEST_LABELS)\n",
    "\n",
    "X_train = np.matrix(zscore(images_train, axis=1))\n",
    "Y_train = np.matrix([np.array([1 if i == label else 0 \n",
    "                               for i in range(10)]) \n",
    "                     for label in labels_train])\n",
    "X_test = np.matrix(zscore(images_test, axis=1))\n",
    "Y_test = np.matrix([np.array([1 if i == label else 0 \n",
    "                              for i in range(10)]) \n",
    "                    for label in labels_test])\n",
    "\n",
    "def get_mnn_nework():\n",
    "    network = MultiLayerNeuralNetwork(inputs=X_train,\n",
    "                                      outputs=Y_train,\n",
    "                                      learning_rate=0.0003,\n",
    "                                      layers=[784,150,10],\n",
    "                                      activation_fn=sigmoid,\n",
    "                                      activation_derivative_fn=sigmoid_derivate,\n",
    "                                      validation_size=10000)\n",
    "    return network\n",
    "\n",
    "def test_cases():\n",
    "    \n",
    "    #Case d iii)\n",
    "    network = get_mnn_nework()\n",
    "    weights = network.train(mini_batch_learning=True, \n",
    "                            momentum=0, lambda_val=0)\n",
    "\n",
    "    # Case e) \n",
    "    network = get_mnn_nework()\n",
    "    weights = network.train(mini_batch_learning=True, \n",
    "                            momentum=0, \n",
    "                            lambda_val=0.001)\n",
    "    weights = network.train(mini_batch_learning=True, \n",
    "                            momentum=0, lambda_val=0.0001)\n",
    "\n",
    "    # Case f)\n",
    "    network = get_mnn_nework()\n",
    "    weights = network.train(mini_batch_learning=True, \n",
    "                            momentum=0.9, lambda_val=0)\n",
    "\n",
    "    # Case g)\n",
    "    network = get_mnn_nework()\n",
    "    network.activation_fn = tanhx_activation_fn\n",
    "    network.activation_derivative_fn = tanhx_activation_fn_derivative\n",
    "    weights = network.train(mini_batch_learning=True, momentum=0, lambda_val=0)\n",
    "\n",
    "    network.activation_fn = max_activation_fn\n",
    "    network.activation_derivative_fn = max_activation_fn_derivative\n",
    "    weights = network.train(mini_batch_learning=True, momentum=0, lambda_val=0)\n",
    "\n",
    "    # Case h)\n",
    "    # Hidden units are reduced by half i.e to 75.\n",
    "    network = get_mnn_nework()\n",
    "    network.layers=[784,75,10]\n",
    "    weights = network.train(mini_batch_learning=True, momentum=0, lambda_val=0)\n",
    "\n",
    "    # Hidden units are increased by double i.e to 300\n",
    "    network = get_mnn_nework()\n",
    "    network.layers=[784,300,10]\n",
    "    weights = network.train(mini_batch_learning=True, momentum=0, lambda_val=0)\n",
    "\n",
    "\n",
    "    # Hidden units are too low i.e 5(assuming that 5 is very low)\n",
    "    network = get_mnn_nework()\n",
    "    network.layers=[784,5,10]\n",
    "    weights = network.train(mini_batch_learning=True, momentum=0, lambda_val=0)\n",
    "\n",
    "    # Hidden units are too high i.e 10000(assuming that 10000 is high)\n",
    "    network = get_mnn_nework()\n",
    "    network.layers=[784,10000,10]\n",
    "    weights = network.train(mini_batch_learning=True, momentum=0, lambda_val=0)\n",
    "    \n",
    "    # 2 hidden layers.\n",
    "    network = get_mnn_nework()\n",
    "    network.layers=[784,100,100,10]\n",
    "    weights = network.train(mini_batch_learning=True, momentum=0, lambda_val=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
