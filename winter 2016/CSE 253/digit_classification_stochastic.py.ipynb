{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__          import division\n",
    "from scipy.stats         import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import array\n",
    "import math\n",
    "import struct\n",
    "\n",
    "%matplotlib inline\n",
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mnist(images_file, labels_file): \n",
    "    f1 = open(labels_file, 'rb')\n",
    "    magic_number, size = struct.unpack(\">II\", f1.read(8))\n",
    "    labels = array.array(\"b\", f1.read())\n",
    "    f1.close()\n",
    "    \n",
    "    f2 = open(images_file, 'rb')\n",
    "    magic_number, size, rows, cols = struct.unpack(\">IIII\", f2.read(16))\n",
    "    raw_images = array.array(\"B\", f2.read())\n",
    "    f2.close()\n",
    "\n",
    "    N = len(labels)\n",
    "    images = np.zeros((N, rows*cols), dtype=np.uint8)\n",
    "    for i in range(N):\n",
    "        images[i] = np.array(raw_images[ i*rows*cols : (i+1)*rows*cols ])\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Read Training data.\n",
    "TRAIN_IMAGES  = \"C:\\\\Users\\\\oop\\\\Desktop\\\\Winter 2016\\\\train-images.idx3-ubyte\"\n",
    "TRAIN_LABELS  = \"C:\\\\Users\\\\oop\\\\Desktop\\\\Winter 2016\\\\train-labels.idx1-ubyte\"\n",
    "images_train, labels_train = read_mnist(TRAIN_IMAGES, TRAIN_LABELS)\n",
    "#images_train, labels_train = images_train[:20000], labels_train[:20000]\n",
    "\n",
    "# Read Test data.\n",
    "TEST_IMAGES = \"C:\\\\Users\\\\oop\\\\Desktop\\\\Winter 2016\\\\t10k-images.idx3-ubyte\"\n",
    "TEST_LABELS  = \"C:\\\\Users\\\\oop\\\\Desktop\\\\Winter 2016\\\\t10k-labels.idx1-ubyte\"\n",
    "images_test, labels_test = read_mnist(TEST_IMAGES, TEST_LABELS)\n",
    "#images_test, labels_test = images_test[:2000], labels_test[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if type(x) is np.ndarray or type(x) is list:\n",
    "        return np.array([sigmoid(ele) for ele in x])\n",
    "    else:\n",
    "        return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivate(x):\n",
    "    if type(x) is np.ndarray or type(x) is list:\n",
    "        return np.array([sigmoid_derivate(ele) for ele in x])\n",
    "    else:\n",
    "        a = sigmoid(x)\n",
    "        return a * (1-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.matrix(zscore(images_train, axis=1))\n",
    "Y_train = np.matrix([np.array([1 if i == label else 0 for i in range(10)]) for label in labels_train])\n",
    "X_test = np.matrix(zscore(images_test, axis=1))\n",
    "Y_test = np.matrix([np.array([1 if i == label else 0 for i in range(10)]) for label in labels_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiLayerNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, inputs, outputs, learning_rate, layers, \n",
    "                 activation_fn, activation_derivative_fn, \n",
    "                 validation_size, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        TODO: Add doc.\n",
    "        \"\"\"\n",
    "        train_data_size = len(inputs) - validation_size\n",
    "        self.inputs = inputs[:train_data_size]\n",
    "        self.outputs = outputs[:train_data_size]\n",
    "        self.cross_validation_inputs = inputs[train_data_size:]\n",
    "        self.cross_validation_outputs = outputs[train_data_size:]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = layers\n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_derivative_fn = activation_derivative_fn\n",
    "    \n",
    "    def get_random_weights(self):\n",
    "        weights = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            weights.append(np.random.random((self.layers[i]+1, \n",
    "                                             self.layers[i+1])))\n",
    "        return weights\n",
    "    \n",
    "    def get_gradients(self, weights):\n",
    "        # Forward propogation.\n",
    "        \n",
    "        i = random.randint(0,len(self.inputs)-1)\n",
    "        X = self.inputs\n",
    "        Y = self.outputs\n",
    "        a,z = self.forward_prop(X, Y, weights)\n",
    "            \n",
    "        # Backward error propogation.\n",
    "        deltas = []\n",
    "        deltas.append((Y - a[-1]))\n",
    "        for l in reversed(range(1, len(self.layers)-1)):\n",
    "            g = np.apply_along_axis(self.activation_derivative_fn, axis=1, arr=z[l])\n",
    "            delta = np.matrix(np.array(np.matmul(deltas[-1], weights[l].transpose()))*np.array(g))\n",
    "            deltas.append(np.delete(delta, 0, axis=1)) # Note that we remove deltas calculated for bias node.\n",
    "        deltas.reverse()\n",
    "            \n",
    "        gradients = []\n",
    "        for i in range(len(weights)):\n",
    "            gradients.append(np.matmul(a[i].transpose(), deltas[i]))\n",
    "        return gradients\n",
    "            \n",
    "    def train(self, weights=None, iterations=100):\n",
    "        \"\"\"\n",
    "        Trains the data using multilayered.\n",
    "        \"\"\"\n",
    "        weights     = weights or self.get_random_weights()\n",
    "        plot_points = [0,50,100,200,300]\n",
    "        train_error = []\n",
    "        test_error  = []\n",
    "        k = 0\n",
    "        # On-line Learning.\n",
    "        for itr in range(0, iterations+1):\n",
    "            \n",
    "            if itr != 0:\n",
    "                # Use gradient descent algorithm to update\n",
    "                # accordingly due to error derivatives.\n",
    "                gradients = self.get_gradients(weights)\n",
    "                for i in range(len(weights)):\n",
    "                    weights[i] = weights[i] + self.learning_rate * gradients[i]\n",
    "                    \n",
    "            if itr in plot_points:\n",
    "                error = self.test(self.inputs, \n",
    "                                  self.outputs,\n",
    "                                  weights)\n",
    "                print error\n",
    "                train_error.append(error)\n",
    "                error = self.test(self.cross_validation_inputs, \n",
    "                                  self.cross_validation_outputs,\n",
    "                                  weights)\n",
    "                print error\n",
    "                test_error.append(error)\n",
    "                \n",
    "        self.weights     = weights\n",
    "        self.train_error = train_error\n",
    "        self.test_error  = test_error\n",
    "        \n",
    "        return weights\n",
    "        \n",
    "    def forward_prop(self, inputs, outputs, weights):\n",
    "        \"\"\"\n",
    "        Calculates the output at each layer of the network.\n",
    "        \"\"\"\n",
    "        a = [np.insert(inputs, 0, 1, axis=1)]\n",
    "        z = [np.insert(inputs, 0, 1, axis=1)]\n",
    "        for l in range(len(self.layers)-1):\n",
    "            zl = np.matmul(z[l], weights[l])\n",
    "            if l == (len(self.layers)-2):\n",
    "                output = np.exp(zl)\n",
    "                output = output / output.sum(axis=1)\n",
    "                a.append(output)\n",
    "                z.append(zl)\n",
    "            else:\n",
    "                z.append(np.insert(zl, 0, 1, axis=1))\n",
    "                al = np.apply_along_axis(self.activation_fn, axis=1, arr=zl)\n",
    "                a.append(np.insert(al, 0, 1, axis=1))\n",
    "        return a,z\n",
    "    \n",
    "    def test(self, test_inputs, test_outputs, weights):\n",
    "        a,z = self.forward_prop(test_inputs, test_outputs, weights)\n",
    "        predicted_digits = a[-1].argmax(axis=1)\n",
    "        actual_digits = test_outputs.argmax(axis=1)\n",
    "        error = (predicted_digits != actual_digits).sum()\n",
    "        return error * 100 / len(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = MultiLayerNeuralNetwork(inputs=X_train,\n",
    "                                  outputs=Y_train,\n",
    "                                  learning_rate=0.001,\n",
    "                                  layers=[784,150,10],\n",
    "                                  activation_fn=sigmoid,\n",
    "                                  activation_derivative_fn=sigmoid_derivate,\n",
    "                                  validation_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.848\n",
      "88.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:94: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:95: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-256fbe0a7988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-125-fc6e9ac00897>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, weights, iterations)\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;31m# Use gradient descent algorithm to update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;31m# accordingly due to error derivatives.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                     \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-125-fc6e9ac00897>\u001b[0m in \u001b[0;36mget_gradients\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weights = network.train(iterations=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
